{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Visual Attention \n",
    "\n",
    "## Learning Objectives\n",
    "1. Learn how to create an image captioning model\n",
    "2. Learn how to train and predict a text generation model.\n",
    "\n",
    "Image captioning models take an image as input, and output text. Ideally, we want the output of the model to accurately describe the events/things in the image, similar to a caption a human might provide. <br>\n",
    "For example, given an image like the example below, the model is expected to generate a caption such as *\"some people are playing baseball.\"*.\n",
    "\n",
    "<div><img src=\"./sample_images/baseball.jpeg\" width=\"500\"></div>\n",
    "\n",
    "In order to generate text, we will build an encoder-decoder model, where the encoder output embedding of an input image, and the decoder output text from the image embedding<br>\n",
    "\n",
    "I this notebook, we will use the model architecture similar to [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), and build Attention-based image captioning model.\n",
    "\n",
    "This notebook is an end-to-end example. The training dataset is the COCO large-scale object detection, segmentation, and captioning dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G20XL3WTBXhj",
    "outputId": "a928c30a-f6d6-4120-9da1-f4f5753b9821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import ( #These are all of the layers we'll use for image captioning.\n",
    "    GRU,\n",
    "    Add,\n",
    "    AdditiveAttention,\n",
    "    Attention,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    LayerNormalization,\n",
    "    Reshape,\n",
    "    StringLookup,\n",
    "    TextVectorization,\n",
    ")\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr6sJGEIBe7D"
   },
   "source": [
    "## Read and prepare dataset\n",
    "\n",
    "We will use the TensorFlow datasets capability to read the [COCO captions](https://www.tensorflow.org/datasets/catalog/coco_captions) dataset.\n",
    "This version contains images, bounding boxes, labels, and captions from COCO 2014, split into the subsets defined by Karpathy and Li (2015) and takes\n",
    "care of some data quality issues with the original dataset (for example, some\n",
    "of the images in the original dataset did not have captions)\n",
    "\n",
    "First, let's define some constants.<br>\n",
    "In this lab, we will use a pretrained [InceptionResNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2/InceptionResNetV2) model from `tf.keras.applications` as a feature extractor, so some constants are comming from the InceptionResNetV2 model definition.<br>\n",
    "So if you want to use other type of base model, please make sure to change these constants as well.\n",
    "\n",
    "`tf.keras.applications` is a pretrained model repository like [TensorFlow Hub](https://tfhub.dev), but while Tensorflow Hub hosts models for different modalities including image, text, audio, and so on, `tf.keras.application` only hosts popular and stable models for images.<br>\n",
    "However, `tf.keras.applications` is more flexible as it contains model metadata and it allow us to access and control the model behavior, while most of the TensorFlow Hub based models that only contains compiled SavedModels.<br>\n",
    "So, for example, we can get output not only from the final layer of the model (e.g. flattend 1D Tensor output of CNN models), but also from intermediate layers (e.g. intermediate 3D Tensor) by accessing layer metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U2WQtNeGBbMD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219055592/219055592 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Change these to control the accuracy/speed\n",
    "VOCAB_SIZE = 20000  # use fewer words to speed up convergence\n",
    "ATTENTION_DIM = 512  # size of dense layer in Attention\n",
    "WORD_EMBEDDING_DIM = 128\n",
    "\n",
    "# InceptionResNetV2 takes (299, 299, 3) image as inputs\n",
    "# and return features in (8, 8, 1536) shape\n",
    "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2( #This is a classical CNN-based model.\n",
    "    include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "IMG_CHANNELS = 3\n",
    "FEATURES_SHAPE = (8, 8, 1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Preprocess\n",
    "Here we preprocess the dataset. The function below:\n",
    "- resize image to (`IMG_HEIGHT`, `IMG_WIDTH`) shape\n",
    "- rescale pixel values from [0, 255] to [0, 1]\n",
    "- return image(`image_tensor`) and captions(`captions`) dictionary.\n",
    "\n",
    "**Note**: This dataset is too large to store in an local environment. Therefore, It is stored in a public GCS bucket located in us-central1. \n",
    "So if you access it from a Notebook outside the US, it will be (a) slow and (b) subject to a network charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8JxC6DhwAcw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:/Users/Dave/Desktop/pet_projects/gen_ai_lab_work/gcp_ailabenv/Lib/site-packages/tensorflow_datasets\\coco_captions\\2014\\1.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e7495500cf4d6bbfe04645e8ff068f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf75edf87d0b472ca363720003ff2a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf90044e3d14044b4a3ad8ea0f8a38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Dave\\\\Desktop\\\\pet_projects\\\\gen_ai_lab_work\\\\gcp_ailabenv\\\\Lib\\\\site-packages\\\\tensorflow_datasets\\\\downloads\\\\images.cocoda.org_annota_image_info_test20PJLS9uJ_Z2n42SD2EkxPQ7vTIOGQY5MIEHTA77ORYaw.zip.tmp.bc38e5f0792d43a885b7567969328778\\\\image_info_test2015.zip' -> 'C:\\\\Users\\\\Dave\\\\Desktop\\\\pet_projects\\\\gen_ai_lab_work\\\\gcp_ailabenv\\\\Lib\\\\site-packages\\\\tensorflow_datasets\\\\downloads\\\\images.cocoda.org_annota_image_info_test20z0ACQvhJclf7ij42m8dmSR9KfkJiX7PXJVVQTpqMOxg.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     img \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m: img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m: caption}\n\u001b[1;32m---> 13\u001b[0m trainds \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoco_captions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGCS_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainds \u001b[38;5;241m=\u001b[39m trainds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     16\u001b[0m     get_image_label, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[0;32m     17\u001b[0m )\u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE)\n\u001b[0;32m     18\u001b[0m trainds \u001b[38;5;241m=\u001b[39m trainds\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:640\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    635\u001b[0m     name,\n\u001b[0;32m    636\u001b[0m     data_dir,\n\u001b[0;32m    637\u001b[0m     builder_kwargs,\n\u001b[0;32m    638\u001b[0m     try_gcs,\n\u001b[0;32m    639\u001b[0m )\n\u001b[1;32m--> 640\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 499\u001b[0m   dbuilder\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs)\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 646\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    652\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    653\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    654\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1498\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1497\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1498\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(  \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m     dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptional_pipeline_kwargs\n\u001b[0;32m   1500\u001b[0m )\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1505\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1506\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1507\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1508\u001b[0m )\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\object_detection\\coco_captions.py:86\u001b[0m, in \u001b[0;36mCocoCaptions._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Only keep the coco train and validation sets.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m coco_splits \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m     split\u001b[38;5;241m.\u001b[39mname: split\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCocoCaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m }\n\u001b[0;32m     88\u001b[0m coco_train_split \u001b[38;5;241m=\u001b[39m coco_splits[tfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN]\n\u001b[0;32m     89\u001b[0m coco_val_split \u001b[38;5;241m=\u001b[39m coco_splits[tfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mVALIDATION]\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\object_detection\\coco.py:245\u001b[0m, in \u001b[0;36mCoco._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# DownloadManager memoize the url, so duplicate urls will only be downloaded\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# once.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m root_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://images.cocodataset.org/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 245\u001b[0m extracted_paths \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m splits \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_config\u001b[38;5;241m.\u001b[39msplits:\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:687\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m    686\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:830\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m    830\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[0;32m    832\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\promise\\promise.py:87\u001b[0m, in \u001b[0;36mtry_catch\u001b[1;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_catch\u001b[39m(handler, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (handler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     89\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:407\u001b[0m, in \u001b[0;36mDownloadManager._download.<locals>.<lambda>\u001b[1;34m(dl_result)\u001b[0m\n\u001b[0;32m    401\u001b[0m   future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    402\u001b[0m       url, download_tmp_dir, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_ssl\n\u001b[0;32m    403\u001b[0m   )\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Post-process the result\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mthen(\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m dl_result: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_or_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m )\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:472\u001b[0m, in \u001b[0;36mDownloadManager._register_or_validate_checksums\u001b[1;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m   \u001b[38;5;66;03m# Eventually validate checksums\u001b[39;00m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m   \u001b[38;5;66;03m#   download). This is expected as it might mean the downloaded file\u001b[39;00m\n\u001b[0;32m    463\u001b[0m   \u001b[38;5;66;03m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m   _validate_checksums(\n\u001b[0;32m    465\u001b[0m       url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    466\u001b[0m       path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m       force_checksums_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_force_checksums_validation,\n\u001b[0;32m    470\u001b[0m   )\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rename_and_get_final_dl_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputed_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchecksum_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:518\u001b[0m, in \u001b[0;36mDownloadManager._rename_and_get_final_dl_path\u001b[1;34m(self, url, path, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[0;32m    510\u001b[0m dst_path \u001b[38;5;241m=\u001b[39m checksum_path \u001b[38;5;129;01mor\u001b[39;00m url_path\n\u001b[0;32m    511\u001b[0m resource_lib\u001b[38;5;241m.\u001b[39mwrite_info_file(\n\u001b[0;32m    512\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    513\u001b[0m     path\u001b[38;5;241m=\u001b[39mdst_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     url_info\u001b[38;5;241m=\u001b[39mcomputed_url_info,\n\u001b[0;32m    517\u001b[0m )\n\u001b[1;32m--> 518\u001b[0m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mrmdir()  \u001b[38;5;66;03m# Cleanup tmp dir (will fail if dir not empty)\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst_path\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\etils\\epath\\gpath.py:245\u001b[0m, in \u001b[0;36m_GPath.replace\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m    243\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(target)\n\u001b[0;32m    244\u001b[0m backend \u001b[38;5;241m=\u001b[39m _get_backend(\u001b[38;5;28mself\u001b[39m, target)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "File \u001b[1;32m~\\Desktop\\pet_projects\\gen_ai_lab_work\\gcp_ailabenv\\lib\\site-packages\\etils\\epath\\backend.py:162\u001b[0m, in \u001b[0;36m_OsPathBackend.replace\u001b[1;34m(self, path, dst)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot overwrite: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a directory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Dave\\\\Desktop\\\\pet_projects\\\\gen_ai_lab_work\\\\gcp_ailabenv\\\\Lib\\\\site-packages\\\\tensorflow_datasets\\\\downloads\\\\images.cocoda.org_annota_image_info_test20PJLS9uJ_Z2n42SD2EkxPQ7vTIOGQY5MIEHTA77ORYaw.zip.tmp.bc38e5f0792d43a885b7567969328778\\\\image_info_test2015.zip' -> 'C:\\\\Users\\\\Dave\\\\Desktop\\\\pet_projects\\\\gen_ai_lab_work\\\\gcp_ailabenv\\\\Lib\\\\site-packages\\\\tensorflow_datasets\\\\downloads\\\\images.cocoda.org_annota_image_info_test20z0ACQvhJclf7ij42m8dmSR9KfkJiX7PXJVVQTpqMOxg.zip'"
     ]
    }
   ],
   "source": [
    "GCS_DIR = \"C:/Users/Dave/Desktop/pet_projects/gen_ai_lab_work/gcp_ailabenv/Lib/site-packages/tensorflow_datasets\"\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "def get_image_label(example):\n",
    "    caption = example[\"captions\"][\"text\"][0]  # only the first caption per image\n",
    "    img = example[\"image\"]\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img / 255\n",
    "    return {\"image_tensor\": img, \"caption\": caption}\n",
    "\n",
    "\n",
    "trainds = tfds.load(\"coco_captions\", split=\"train\", data_dir=GCS_DIR)\n",
    "\n",
    "trainds = trainds.map(\n",
    "    get_image_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").shuffle(BUFFER_SIZE)\n",
    "trainds = trainds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \n",
    "Let's take a look at images and sample captions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "KGz2bQaKV3iI",
    "outputId": "4bd05dee-e0f5-4934-ac0d-2afa0a20bad8"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, data in enumerate(trainds.take(4)):\n",
    "    ax[idx].imshow(data[\"image_tensor\"].numpy())\n",
    "    caption = \"\\n\".join(wrap(data[\"caption\"].numpy().decode(\"utf-8\"), 30))\n",
    "    ax[idx].set_title(caption)\n",
    "    ax[idx].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4dyKHB2W4vZ"
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We add special tokens to represent the starts (`<start>`) and the ends (`<end>`) of sentences.<br>\n",
    "Start and end tokens are added here because we are using an encoder-decoder model and during prediction, to get the captioning started we use `<start>` and since captions are of variable length, we terminate the prediction when we see the `<end>` token.\n",
    "\n",
    "Then create a full list of the captions for further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_token(data):\n",
    "    start = tf.convert_to_tensor(\"<start>\")\n",
    "    end = tf.convert_to_tensor(\"<end>\")\n",
    "    data[\"caption\"] = tf.strings.join(\n",
    "        [start, data[\"caption\"], end], separator=\" \"\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "trainds = trainds.map(add_start_end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize the captions\n",
    "\n",
    "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
    "\n",
    "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top `VOCAB_SIZE` words.\n",
    "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to the length `MAX_CAPTION_LEN`. Here we directly specify `64` number which is sufficient for this dataset, but please note that this value should be computed by processing the entire dataset if you don't want to cut down very long sentense in a dataset.\n",
    "\n",
    "**Note**: This process takes around 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the `TextVectorization` layer referring to [the document](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization?version=nightly).\n",
    "\n",
    "**Hint**: You can use `VOCAB_SIZE` and `MAX_CAPTION_LEN` variables, and `standardize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2FW4ob5NikW",
    "outputId": "60f31007-6f03-4136-886a-0f9447b0c12b"
   },
   "outputs": [],
   "source": [
    "MAX_CAPTION_LEN = 64\n",
    "\n",
    "\n",
    "# We will override the default standardization of TextVectorization to preserve\n",
    "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
    "def standardize(inputs):\n",
    "    inputs = tf.strings.lower(inputs)\n",
    "    return tf.strings.regex_replace(\n",
    "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Choose the most frequent words from the vocabulary & remove punctuation etc.\n",
    "# TODO: Complete the TextVectorization layer.\n",
    "tokenizer = TextVectorization(...)\n",
    "\n",
    "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's try to tokenize a sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"<start> This is a sentence <end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_captions = []\n",
    "for d in trainds.take(5):\n",
    "    sample_captions.append(d[\"caption\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(sample_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that all the sentenses starts and ends with the same token (e.g. '3' and '4'). These values represent start tokens and end tokens respectively.\n",
    "\n",
    "You can also convert ids to original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wordid in tokenizer([sample_captions[0]])[0]:\n",
    "    print(tokenizer.get_vocabulary()[wordid], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can create Word <-> Index converters using `StringLookup` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: Word -> Index\n",
    "word_to_index = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
    ")\n",
    "\n",
    "# Lookup table: Index -> Word\n",
    "index_to_word = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset for training\n",
    "Now Let's apply the adapted tokenization to all the examples and create tf.data Dataset for training.\n",
    "\n",
    "Here note that we are also creating labels by shifting texts from feature captions.<br>\n",
    "If we have an input caption `\"<start> I love cats <end>\"`, its label should be `\"I love cats <end> <padding>\"`.<br>\n",
    "With that, our model can try to learn to predict `I` from `<start>`.\n",
    "\n",
    "The dataset should return tuples, where the first elements are features (`image_tensor` and `caption`) and the second elements are labels (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Define `create_ds_fn` to create the dataset. It should have the steps below:\n",
    "- Roll the caption by one step using `tf.roll`. The rolled caption will be used as a target.\n",
    "- Add zero value (padding) to the rolled caption to create the same length vector as the original caption.\n",
    "- Return the features (image and original caption) and the target (rolled caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0LHFYjhBo32",
    "outputId": "647be30d-0357-49a2-a49b-010b59696ed0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def create_ds_fn(data):\n",
    "    img_tensor = data[\"image_tensor\"]\n",
    "    caption = tokenizer(data[\"caption\"])\n",
    "\n",
    "    # TODO: Define the steps.\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "batched_ds = (\n",
    "    trainds.map(create_ds_fn)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, caption), label in batched_ds.take(2):\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Caption shape: {caption.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "    print(caption[0])\n",
    "    print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ehA-gDDYh47"
   },
   "source": [
    "## Model\n",
    "Now let's design an image captioning model.<br>\n",
    "It consists of an image encoder, followed by a caption decoder.\n",
    "\n",
    "### Image Encoder\n",
    "The image encoder model is very simple. It extracts features through a pre-trained model and passes them to a fully connected layer.\n",
    "\n",
    "1. In this example, we extract the features from convolutional layers of InceptionResNetV2 which gives us a vector of (Batch Size, 8, 8, 1536).\n",
    "1. We reshape the vector to (Batch Size, 64, 1536)\n",
    "1. We squash it to a length of `ATTENTION_DIM` with a Dense Layer and return (Batch Size, 64, ATTENTION_DIM)\n",
    "1. Later, the Attention layer attends over the image to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_EXTRACTOR.trainable = False\n",
    "\n",
    "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "image_features = FEATURE_EXTRACTOR(image_input)\n",
    "\n",
    "x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(\n",
    "    image_features\n",
    ")\n",
    "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Decoder\n",
    "The caption decoder incorporates an attention mechanism that focuses on different parts of the input image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention head\n",
    "\n",
    "The decoder uses attention to selectively focus on parts of the input sequence.\n",
    "The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. \n",
    "\n",
    "Let's look at how this works:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408554-d4b6387b-248b-421e-8911-550d0561d001.png\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408648-38c6b582-a68b-4697-982a-1d885b83dd0b.png\" alt=\"attention equation 2\" width=\"800\">\n",
    "\n",
    "Where:\n",
    "\n",
    "* $s$ is the encoder index.\n",
    "* $t$ is the decoder index.\n",
    "* $\\alpha_{ts}$ is the attention weights.\n",
    "* $h_s$ is the sequence of encoder outputs being attended to (the attention \"key\" and \"value\" in transformer terminology).\n",
    "* $h_t$ is the decoder state attending to the sequence (the attention \"query\" in transformer terminology).\n",
    "* $c_t$ is the resulting context vector.\n",
    "* $a_t$ is the final output combining the \"context\" and \"query\".\n",
    "\n",
    "The equations:\n",
    "\n",
    "1. Calculates the attention weights, $\\alpha_{ts}$, as a softmax across the encoder's output sequence.\n",
    "2. Calculates the context vector as the weighted sum of the encoder outputs.\n",
    "\n",
    "Last is the $score$ function. Its job is to calculate a scalar logit-score for each key-query pair. There are two common approaches:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408773-3781cacc-de00-49c6-9909-f6cd65a0501b.png\" alt=\"attention equation 4\" width=\"800\">\n",
    "\n",
    "This notebook implement Luong-style attention using pre-defined `layers.Attention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Steps\n",
    "\n",
    "The decoder's job is to generate predictions for the next output token.\n",
    "\n",
    "1. The decoder receives current word tokens as a batch.\n",
    "1. It embeds the word tokens to `ATTENTION_DIM` dimension.\n",
    "1. GRU layer keeps track of the word embeddings, and returns GRU outputs and states.\n",
    "1. Bahdanau-style attention attends over the encoder's output feature by using GRU outputs as a query.\n",
    "1. The attention outputs and GRU outputs are added (skip connection), and normalized in a layer normalization layer.\n",
    "1. It generates logit predictions for the next token based on the GRU output.\n",
    "\n",
    "We can define all the steps in Keras Functional API, but please note that here we instantiate layers that have trainable parameters so that we reuse the layers and the weights in inference phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Define the decoder steps following the instructions above.\n",
    "\n",
    "**Reference**:\n",
    "- [Embedding Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding?version=nightly)\n",
    "- [GPU Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "- [Attention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(MAX_CAPTION_LEN), name=\"words\")\n",
    "\n",
    "# TODO: Define the Embedding layer\n",
    "embed_x = Embedding(...)(word_input)\n",
    "\n",
    "# TODO: Define the GRU layer.\n",
    "decoder_gru = GRU(...)\n",
    "gru_output, gru_state = decoder_gru(embed_x)\n",
    "\n",
    "decoder_atention = Attention()\n",
    "# TODO: Define the inputs to the Attention layer\n",
    "context_vector = decoder_atention([...])\n",
    "\n",
    "addition = Add()([gru_output, context_vector])\n",
    "\n",
    "layer_norm = LayerNormalization(axis=-1)\n",
    "layer_norm_out = layer_norm(addition)\n",
    "\n",
    "decoder_output_dense = Dense(VOCAB_SIZE)\n",
    "decoder_output = decoder_output_dense(layer_norm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.Model(\n",
    "    inputs=[word_input, encoder_output], outputs=decoder_output\n",
    ")\n",
    "tf.keras.utils.plot_model(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "Now we defined the encoder and the decoder. Let's combine them into an image model for training.<br>\n",
    "It has two inputs (`image_input` and `word_input`, and an output (`decoder_output`). This definition should correspond to the definition of the dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_train_model = tf.keras.Model(\n",
    "    inputs=[image_input, word_input], outputs=decoder_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The loss function is a simple cross-entropy, but we need to remove padding (`0`) when calculating it.<br>\n",
    "So here we extract the length of the sentence (non-0 part), and compute the average of the loss only over the valid sentence part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int32)\n",
    "    sentence_len = tf.reduce_sum(mask)\n",
    "    loss_ = loss_[:sentence_len]\n",
    "\n",
    "    return tf.reduce_mean(loss_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_train_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGVl8cQpZ5Qu"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Now we can train the model using the standard `model.fit` API.<br>\n",
    "It takes around 15-20 minutes with NVIDIA T4 GPU to train 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = image_caption_train_model.fit(batched_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otiuFI4ZaK6w"
   },
   "source": [
    "## Caption!\n",
    "\n",
    "The predict step is different from the training, since we need to keep track of the GRU state during the caption generation, and pass a predicted word to the decoder as an input at the next time step.\n",
    "\n",
    "In order to do so, let's define another model for prediction while using the trained weights, so that it can keep and update the GRU state during the caption generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the decoder for prediction.\n",
    "\n",
    "**Hint**: Most parts of the architecture are the same as training, but this prediction model has additional I/O to pass through the GRU state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the entire prediction model referring to the traiing model above.\n",
    "\n",
    "gru_state_input = Input(shape=(ATTENTION_DIM), name=\"gru_state_input\")\n",
    "\n",
    "# Reuse trained GRU, but update it so that it can receive states.\n",
    "gru_output, gru_state = decoder_gru(...)\n",
    "\n",
    "# Reuse other layers as well\n",
    "context_vector = decoder_atention([...])\n",
    "addition_output = Add()([...])\n",
    "layer_norm_output = layer_norm(...)\n",
    "\n",
    "decoder_output = decoder_output_dense(...)\n",
    "\n",
    "# Define prediction Model with state input and output\n",
    "decoder_pred_model = tf.keras.Model(\n",
    "    inputs=[...],\n",
    "    outputs=[...],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Initialize the GRU states as zero vectors.\n",
    "1. Preprocess an input image, pass it to the encoder, and extract image features.\n",
    "1. Setup word tokens of `<start>` to start captioning.\n",
    "1. In the for loop, we\n",
    "    - pass word tokens (`dec_input`), GRU states (`gru_state`) and image features (`features`) to the prediction decoder and get predictions (`predictions`), and the updated GRU states.\n",
    "    - select Top-K words from logits, and choose a word probabilistically so that we avoid computing softmax over VOCAB_SIZE-sized vector.\n",
    "    - stop predicting when the model predicts the `<end>` token.\n",
    "    - replace the input word token with the predicted word token for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkmKr8nxMNyG"
   },
   "outputs": [],
   "source": [
    "MINIMUM_SENTENCE_LENGTH = 5\n",
    "\n",
    "\n",
    "## Probabilistic prediction using the trained model\n",
    "def predict_caption(filename):\n",
    "    gru_state = tf.zeros((1, ATTENTION_DIM))\n",
    "\n",
    "    img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img / 255\n",
    "\n",
    "    features = encoder(tf.expand_dims(img, axis=0))\n",
    "    dec_input = tf.expand_dims([word_to_index(\"<start>\")], 1)\n",
    "    result = []\n",
    "    for i in range(MAX_CAPTION_LEN):\n",
    "        predictions, gru_state = decoder_pred_model(\n",
    "            [dec_input, gru_state, features]\n",
    "        )\n",
    "\n",
    "        # draws from log distribution given by predictions\n",
    "        top_probs, top_idxs = tf.math.top_k(\n",
    "            input=predictions[0][0], k=10, sorted=False\n",
    "        )\n",
    "        chosen_id = tf.random.categorical([top_probs], 1)[0].numpy()\n",
    "        predicted_id = top_idxs.numpy()[chosen_id][0]\n",
    "\n",
    "        result.append(tokenizer.get_vocabulary()[predicted_id])\n",
    "\n",
    "        if predicted_id == word_to_index(\"<end>\"):\n",
    "            return img, result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caption! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../sample_images/baseball.jpeg\"  # you can also try surf.jpeg\n",
    "\n",
    "for i in range(5):\n",
    "    image, caption = predict_caption(filename)\n",
    "    print(\" \".join(caption[:-1]) + \".\")\n",
    "\n",
    "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look?<br>\n",
    "It seems the model captures the key aspects of the image: people, baseball, and a ball, although it could not be grammatically perfect.\n",
    "\n",
    "### Optional Task:\n",
    "Upload your own image and generate captions with it.<br>\n",
    "Also, you can try to train longer to achieve better captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We learned how to build an image captioning model by creating an image encoder and a text decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_fNzWuY2UoB"
   },
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fa3LRWv9BNk9",
    "y4dyKHB2W4vZ",
    "1ehA-gDDYh47"
   ],
   "name": "laks_work.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
